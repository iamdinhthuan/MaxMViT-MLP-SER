# MaxMViT-MLP vá»›i GMU Fusion (GloMER-inspired)

## ğŸ“Š So sÃ¡nh Fusion Strategies

### Original MaxMViT-MLP (Concatenation)
```
CQT â†’ MaxViT â†’ [feat_cqt]
                           â†’ concat â†’ [feat_cqt; feat_mel] â†’ MLP â†’ output
Mel-STFT â†’ MViTv2 â†’ [feat_mel]
```

**Váº¥n Ä‘á»:**
- Static fusion: khÃ´ng adapt theo input
- KhÃ´ng kiá»ƒm soÃ¡t modality dominance
- Feature dimension lá»›n (dim_cqt + dim_mel)

### Improved (GMU Fusion)
```
CQT â†’ MaxViT â†’ [feat_cqt] â†’ tanh projection â†’ zÌƒ_cqt
                                                        â†’ GMU â†’ z_fused â†’ MLP
Mel-STFT â†’ MViTv2 â†’ [feat_mel] â†’ tanh projection â†’ zÌƒ_mel

GMU: g = Ïƒ(W[zÌƒ_cqt; zÌƒ_mel])
     z_fused = g âŠ™ zÌƒ_cqt + (1-g) âŠ™ zÌƒ_mel
```

**Æ¯u Ä‘iá»ƒm:**
- Dynamic fusion: tá»± Ä‘iá»u chá»‰nh weight theo input
- Balanced modality contribution
- Smaller feature dimension
- Interpretable (xem gate values)

---

## ğŸ”‘ GMU - CÃ¡ch hoáº¡t Ä‘á»™ng

```python
# Gate vector há»c cÃ¡ch cÃ¢n báº±ng 2 modalities
g = sigmoid(W_gate @ concat(z_cqt, z_mel))  # g âˆˆ [0, 1]

# Adaptive fusion
z_fused = g * z_cqt + (1 - g) * z_mel

# Interpretation:
# g â†’ 1.0: Model tin CQT/MaxViT path nhiá»u hÆ¡n
# g â†’ 0.0: Model tin Mel-STFT/MViTv2 path nhiá»u hÆ¡n
# g â‰ˆ 0.5: Balanced fusion
```

---

## ğŸ“ˆ Expected Results

| Model | IEMOCAP (paper) | Fusion Type |
|-------|-----------------|-------------|
| MaxMViT-MLP (original) | 68.39% | Concatenation |
| MaxMViT-MLP + GMU | ~70-72% (expected) | Gated Fusion |
| MaxMViT-MLP + GMU + Contrastive | ~72-75% (expected) | Gated + Contrastive |

**Note:** GloMER Ä‘áº¡t 82.79% trÃªn IEMOCAP nhÆ°ng há» dÃ¹ng **Text + Audio** (BERT + Wav2Vec).
MaxMViT-MLP dÃ¹ng **CQT + Mel-STFT** (cÃ¹ng source audio), nÃªn improvement sáº½ modest hÆ¡n.

---

## ğŸ› ï¸ CÃ¡ch sá»­ dá»¥ng

### 1. GMU Fusion (recommended Ä‘á»ƒ báº¯t Ä‘áº§u)
```bash
python train_gmu.py --config configs/iemocap_gmu.yaml
```

### 2. GMU + Contrastive Learning
```bash
python train_gmu.py --config configs/iemocap_gmu_contrastive.yaml
```

### 3. So sÃ¡nh vá»›i Original
```bash
# Original (concat fusion)
python train.py --config configs/iemocap.yaml

# New (GMU fusion)
python train_gmu.py --config configs/iemocap_gmu.yaml
```

---

## ğŸ“ File Structure

```
maxmvit_gmu/
â”œâ”€â”€ model_gmu.py                 # GMU model implementation
â”œâ”€â”€ train_gmu.py                 # Training script with GMU support
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ iemocap_gmu.yaml         # GMU only
â”‚   â””â”€â”€ iemocap_gmu_contrastive.yaml  # GMU + Contrastive
â””â”€â”€ FUSION_COMPARISON.md         # This file
```

---

## ğŸ”¬ Ablation Study Guide

Äá»ƒ so sÃ¡nh Ä‘áº§y Ä‘á»§, cháº¡y 3 experiments:

| Experiment | Config | Fusion |
|------------|--------|--------|
| Baseline | `iemocap.yaml` (fix lr=0.02) | concat |
| GMU only | `iemocap_gmu.yaml` | gmu |
| GMU + CL | `iemocap_gmu_contrastive.yaml` | gmu_contrastive |

---

## ğŸ›ï¸ Hyperparameter Tuning

### GMU
- `fusion_hidden_dim`: None (auto) hoáº·c 512, 768, 1024

### Contrastive Learning (tá»« GloMER paper)
- `alpha`: 0.3 (IEMOCAP), 0.5 (ESD), tune trong [0, 1.5]
- `temperature`: 0.07 (standard NT-Xent)

### Learning Rate
- Giá»¯ 0.02 nhÆ° MaxMViT-MLP paper
- GloMER dÃ¹ng 1e-4 nhÆ°ng há» dÃ¹ng pretrained BERT/Wav2Vec

---

## ğŸ“ Key Differences: GloMER vs This Implementation

| Aspect | GloMER | This (MaxMViT-MLP + GMU) |
|--------|--------|--------------------------|
| Modalities | Text (BERT) + Audio (Wav2Vec) | CQT (MaxViT) + Mel-STFT (MViTv2) |
| Information source | 2 different sources | Same audio, 2 representations |
| Cross-modal attention | Yes | No (could add) |
| GMU | Yes âœ“ | Yes âœ“ |
| Contrastive Learning | Yes | Optional âœ“ |
| Expected benefit | High (complementary info) | Moderate (redundant info) |

---

## ğŸš€ Next Steps

1. **Run baseline** vá»›i lr=0.02 fix
2. **Run GMU** vÃ  compare
3. **Analyze gate values** Ä‘á»ƒ hiá»ƒu model behavior
4. **Try contrastive** náº¿u GMU improves
5. **Optional**: Add cross-modal attention (nhÆ° GloMER)
